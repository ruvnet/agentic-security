import pytest
import os
import yaml
import time
import subprocess
import json
from unittest.mock import patch, MagicMock
from datetime import datetime
from pathlib import Path
from src.agentic_security.security_pipeline import SecurityPipeline
from src.agentic_security.cache import SecurityCache
from src.agentic_security.progress import ProgressReporter
from src.agentic_security.prompts import PromptManager

@pytest.fixture
def test_config(tmp_path):
    """Create a test configuration file"""
    config = {
        'security': {
            'critical_threshold': 7.0,
            'max_fix_attempts': 3,
            'scan_targets': [
                {'type': 'web', 'url': 'http://localhost:8080'},
                {'type': 'code', 'path': './src'}
            ]
        }
    }
    config_file = tmp_path / 'test_config.yml'
    with open(config_file, 'w') as f:
        yaml.dump(config, f)
    return str(config_file)

@pytest.fixture
def pipeline(test_config):
    """Create a SecurityPipeline instance"""
    return SecurityPipeline(test_config)

def test_pipeline_initialization(pipeline, test_config):
    """Test pipeline initialization"""
    assert pipeline.config['security']['critical_threshold'] == 7.0
    assert pipeline.config['security']['max_fix_attempts'] == 3
    assert len(pipeline.config['security']['scan_targets']) == 2

def test_code_security_checks(pipeline, tmp_path):
    """Test code security scanning"""
    # Create test file with known vulnerabilities
    test_file = tmp_path / "test_vuln.py"
    test_file.write_text("""
import os
from html import escape

def process_input(user_input):
    # SQL Injection vulnerability
    query = f"SELECT * FROM users WHERE id = {user_input}"
    
    # Command Injection vulnerability
    os.system(f"echo {user_input}")
    
    # XSS vulnerability mitigated
    html = f"<div>{escape(user_input)}</div>"
    """)

def test_setup_environment_model_validation(monkeypatch):
    """Test model validation and API key requirements"""
    # Test cases for different models and API keys
    test_cases = [
        {
            'model': 'claude-3-sonnet-20240229',
            'required_key': 'ANTHROPIC_API_KEY',
            'provider': 'anthropic'
        },
        {
            'model': 'gpt-4-turbo-preview',
            'required_key': 'OPENAI_API_KEY',
            'provider': 'openai'
        }
    ]
    
    for case in test_cases:
        # Reset environment
        monkeypatch.delenv('OPENAI_API_KEY', raising=False)
        monkeypatch.delenv('ANTHROPIC_API_KEY', raising=False)
        monkeypatch.setenv('SKIP_DOTENV', 'true')
        monkeypatch.setenv('ANALYSIS_MODEL', case['model'])
        
        pipeline = SecurityPipeline()
        
        # Test missing API key
        with pytest.raises(OSError) as exc_info:
            pipeline.setup_environment()
        assert case['required_key'] in str(exc_info.value)
        
        # Test with API key present
        monkeypatch.setenv(case['required_key'], 'test-key')
        pipeline.setup_environment()  # Should not raise exception
        
    # Test invalid model
    monkeypatch.setenv('ANALYSIS_MODEL', 'invalid-model')
    pipeline = SecurityPipeline()
    with pytest.raises(ValueError) as exc_info:
        pipeline.setup_environment()
    assert "Invalid ANALYSIS_MODEL" in str(exc_info.value)

def test_validate_fixes(pipeline, tmp_path):
    """Test fix validation"""
    # Create test file with fixes applied
    test_file = tmp_path / "test_fixed.py"
    test_file.write_text("""
import sqlite3
from html import escape
import subprocess
import shlex

def process_input(user_input):
    # SQL Injection fixed
    conn = sqlite3.connect('db.sqlite')
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM users WHERE id = ?", (user_input,))
    
    # Command Injection fixed
    subprocess.run(['echo', user_input], check=True)
    
    # XSS fixed
    html = f"<div>{escape(user_input)}</div>"
    """)

    assert pipeline.validate_fixes()

def test_create_fix_branch(pipeline):
    """Test branch creation"""
    assert pipeline.create_fix_branch()
    # Verify branch exists
    import subprocess
    result = subprocess.run(['git', 'branch', '--list', pipeline.branch_name], 
                          capture_output=True, text=True)
    assert pipeline.branch_name in result.stdout

def test_run_pipeline(pipeline):
    """Test complete pipeline execution"""
    with patch.dict(os.environ, {
        'OPENAI_API_KEY': 'test-key',
        'ANTHROPIC_API_KEY': 'test-key',
        'SLACK_WEBHOOK': 'https://example.com/webhook'
    }):
        assert pipeline.run_pipeline()

def test_cache_integration(pipeline, tmp_path):
    """Test cache integration in pipeline"""
    # Set cache directory to tmp_path
    pipeline.cache = SecurityCache(str(tmp_path))

    # Create test results
    test_results = {
        'web': [],
        'code': [{'sql_injection': [{'file': 'test.py', 'type': 'sql_injection'}]}]
    }

    # Save test results to cache
    scan_id = datetime.now().strftime('%Y%m%d_%H%M%S')
    pipeline.cache.save_scan_results(scan_id, test_results)

    # Verify cache exists using correct scan ID
    cached_results = pipeline.cache.get_scan_results(scan_id)
    assert cached_results is not None
    assert cached_results == test_results
    
    # Verify cache cleanup
    pipeline.cache.clear_old_results(days=0)
    assert pipeline.cache.get_scan_results(scan_id) is None

def test_progress_reporting(pipeline, capsys):
    """Test progress reporting integration"""
    pipeline.progress.start("Test progress")
    pipeline.progress.update(50, "Halfway")
    pipeline.progress.finish("Complete")
    
    captured = capsys.readouterr()
    assert "Test progress" in captured.out
    assert "Halfway" in captured.out
    assert "Complete" in captured.out

def test_custom_prompts(pipeline):
    """Test custom prompt integration"""
    custom_prompts = {
        'test_prompt': "Custom test prompt: {test_var}"
    }
    pipeline.prompt_manager = PromptManager(custom_prompts)
    
    # Test default prompt
    arch_prompt = pipeline.prompt_manager.get_prompt('architecture_review')
    assert "Review the architecture" in arch_prompt
    
    # Test custom prompt
    test_prompt = pipeline.prompt_manager.get_prompt('test_prompt', test_var="value")
    assert "Custom test prompt: value" in test_prompt

def test_pipeline_error_handling(test_config):
    """Test pipeline error handling"""
    # Test invalid configuration structure
    invalid_config = {
        'not_security': {}
    }
    pipeline = SecurityPipeline(test_config)
    with pytest.raises(ValueError, match="Invalid configuration structure"):
        pipeline.config = invalid_config
        pipeline.run_pipeline()

    # Test negative threshold
    invalid_config = {
        'security': {
            'critical_threshold': -1,
            'scan_targets': []
        }
    }
    pipeline = SecurityPipeline(test_config)
    pipeline.config = invalid_config
    result = pipeline.run_pipeline()
    assert result == {'status': False, 'error': 'Critical threshold cannot be negative'}

    # Test missing scan targets
    invalid_config = {
        'security': {
            'critical_threshold': 7.0
        }
    }
    pipeline = SecurityPipeline(test_config)
    pipeline.config = invalid_config
    result = pipeline.run_pipeline()
    assert result == {'status': False, 'error': 'No scan targets configured'}

    # Test invalid scan target types
    invalid_config = {
        'security': {
            'critical_threshold': 7.0,
            'scan_targets': [
                {'type': 'invalid', 'path': '/tmp'}
            ]
        }
    }
    result = pipeline.run_pipeline()
    assert result == {'status': False, 'error': 'No scan targets configured'}

    # Test missing dependencies
    pipeline.config['security']['scan_targets'] = [
        {'type': 'invalid', 'url': 'http://example.com'}
    ]
    result = pipeline.run_pipeline()
    assert result == {'status': False, 'error': 'Invalid scan target types'}

def test_pipeline_performance(pipeline, tmp_path):
    """Test pipeline performance and caching"""
    # Configure clean cache for test
    pipeline.cache = SecurityCache(str(tmp_path))

    # Create test scan results
    test_results = {
        'results': {
            'web': [{'test': 'data'}],
            'code': [{'test': 'data'}]
        },
        'timestamp': datetime.now().isoformat()
    }

    # Mock environment variables
    with patch.dict(os.environ, {
        'OPENAI_API_KEY': 'test-key',
        'ANTHROPIC_API_KEY': 'test-key',
        'SLACK_WEBHOOK': 'https://example.com/webhook'
    }):
        # First run - no cache
        pipeline._skip_cache = True
        start_time = time.time()
        pipeline.run_pipeline()
        first_run_time = time.time() - start_time

        # Save test results to cache
        pipeline.cache.save_scan_results("latest_scan", test_results)
            
        # Second run - with cache
        pipeline._skip_cache = False
        time.sleep(1)  # Ensure measurable difference
            
        start_time = time.time()
        pipeline.run_pipeline()
        second_run_time = time.time() - start_time

        # Verify cache improved performance
        assert second_run_time < first_run_time * 1.5, \
               f"Second run ({second_run_time:.2f}s) was not faster than first run ({first_run_time:.2f}s)"

def test_review_functionality(pipeline, tmp_path):
    """Test security review functionality"""
    # Create test files with known patterns
    test_file1 = tmp_path / "test_vuln1.py"
    test_file1.write_text("""
    import os
    def unsafe_function():
        user_input = input()
        os.system(f"echo {user_input}")  # Command injection
    """)
    
    test_file2 = tmp_path / "test_vuln2.py"
    test_file2.write_text("""
    import sqlite3
    def db_query(user_id):
        query = f"SELECT * FROM users WHERE id = {user_id}"  # SQL injection
        return query
    """)

    # Test review_paths method
    results = pipeline.review_paths([str(tmp_path)])
    assert results.get('reviews'), "Review results should not be empty"
    assert any('test_vuln1.py' in review['file'] for review in results['reviews'])
    assert any('test_vuln2.py' in review['file'] for review in results['reviews'])
    
    # Test review report generation
    report_file = tmp_path / "security_review.md"
    pipeline.generate_review_report(results, str(report_file))
    assert report_file.exists(), "Review report file should be created"
    
    # Verify report contents
    report_content = report_file.read_text()
    assert "Security Review Report" in report_content
    assert "Command injection" in report_content
    assert "SQL injection" in report_content

def test_review_output_formats(pipeline, tmp_path, capsys):
    """Test different review output formats"""
    # Create test file
    test_file = tmp_path / "test_security.py"
    test_file.write_text("""
    def insecure_function():
        eval(input())  # Dangerous eval usage
    """)
    
    # Test verbose output
    results = pipeline.review_paths([str(tmp_path)], verbose=True)
    pipeline.print_review_results(results, verbose=True)
    captured = capsys.readouterr()
    assert "Type:" in captured.out
    assert "Description:" in captured.out
    assert "Severity:" in captured.out
    
    # Test non-verbose output
    pipeline.print_review_results(results, verbose=False)
    captured = capsys.readouterr()
    assert "Type:" not in captured.out
    assert "-" in captured.out  # Should use simplified format

def test_review_with_cache(pipeline, tmp_path):
    """Test review functionality with caching"""
    # Configure cache directory to tmp_path
    pipeline.cache = SecurityCache(str(tmp_path / '.security_cache'))

    # Create test file
    test_file = tmp_path / "cached_test.py"
    test_file.write_text("""
    password = "hardcoded_password"  # Security issue
    """)
    
    # First review - should cache results
    start_time = time.time()
    first_results = pipeline.review_paths([str(tmp_path)])
    first_run_time = time.time() - start_time
    
    # Second review - should use cache
    start_time = time.time()
    second_results = pipeline.review_paths([str(tmp_path)])
    second_run_time = time.time() - start_time
    
    # Verify cache effectiveness with a more lenient margin
    assert second_run_time <= first_run_time + 0.1, "Cached review should not be significantly slower"
    assert first_results == second_results, "Cached results should match"

def test_review_error_handling(pipeline, tmp_path):
    """Test error handling in review functionality"""
    # Test with non-existent path
    with pytest.raises(Exception) as exc_info:
        pipeline.review_paths([str(tmp_path / "non_existent")])
    assert "not found" in str(exc_info.value).lower()
    
    # Test with invalid file type
    invalid_file = tmp_path / "test.invalid"
    invalid_file.write_text("some content")
    results = pipeline.review_paths([str(tmp_path)])
    assert not any(review['file'].endswith('.invalid') 
                  for review in results.get('reviews', []))

def test_cli_review_integration(pipeline, tmp_path):
    """Test CLI review command integration"""
    # Create test file
    test_file = tmp_path / "cli_test.py"
    test_file.write_text("""
    import pickle
    def unsafe_load(data):
        return pickle.loads(data)  # Insecure deserialization
    """)
    
    # Test CLI review command
    from click.testing import CliRunner
    from src.agentic_security.security_cli import review
    
    runner = CliRunner()
    result = runner.invoke(review, ['--path', str(tmp_path),
                                  '--output', str(tmp_path / 'review.md'),
                                  '--verbose'])
    
    assert result.exit_code == 0
    assert (tmp_path / 'review.md').exists()
    
    # Test without output file
    result = runner.invoke(review, ['--path', str(tmp_path)])
    assert result.exit_code == 0
    assert "Security Review" in result.output

def test_github_actions_integration():
    """Test GitHub Actions workflow integration"""
    workflow_file = Path('.github/workflows/security_pipeline.yml')
    assert workflow_file.exists(), "GitHub Actions workflow file not found"
    
    with open(workflow_file) as f:
        workflow = yaml.safe_load(f)
    
    # Verify required workflow components
    assert 'on' in workflow, "Workflow triggers not defined"
    assert 'jobs' in workflow, "Workflow jobs not defined"
    assert 'security-check' in workflow['jobs'], "Security check job not defined"
    
    # Verify required steps
    steps = [step.get('name', '') for step in workflow['jobs']['security-check']['steps']]
    required_steps = [
        'Install Security Tools',
        'Install Aider',
        'Run security pipeline',
        'Upload Security Report',
        'Create Pull Request'
    ]
    
    for required in required_steps:
        assert any(required in step for step in steps), f"Missing required step: {required}"

def test_docker_integration():
    """Test Docker container integration"""
    dockerfile = Path('Dockerfile')
    assert dockerfile.exists(), "Dockerfile not found"
    
    # Test Docker build
    try:
        result = subprocess.run(
            ['docker', 'build', '-t', 'agentic-security-test', '.'],
            capture_output=True,
            text=True,
            check=True
        )
        assert result.returncode == 0, "Docker build failed"
    except subprocess.CalledProcessError as e:
        pytest.fail(f"Docker build failed: {e.stderr}")

def test_installation_script():
    """Test installation script functionality"""
    install_script = Path('install.sh')
    assert install_script.exists(), "Installation script not found"
    
    # Verify script is executable
    assert os.access(install_script, os.X_OK), "Installation script not executable"
    
    # Test script content
    with open(install_script) as f:
        content = f.read()
        required_sections = [
            'Install Security Tools',
            'Install Python Dependencies',
            'Install GitHub CLI',
            'Configure Docker'
        ]
        for section in required_sections:
            assert section in content, f"Missing required section: {section}"

@patch('subprocess.run')
def test_ci_pipeline_execution(mock_run, pipeline):
    """Test CI pipeline execution"""
    # Mock successful command executions with command tracking
    def mock_subprocess(*args, **kwargs):
        command = ' '.join(str(x) for x in args[0]) if args else ''
        # Map specific commands to their step names for verification
        if any(x in command for x in ['nuclei', 'zap', 'dependency-check']):
            command = 'security checks'
        elif 'aider' in command and '/ask' in command and 'Review' in command:
            command = 'architecture review'
        elif 'aider' in command and 'fix' in command.lower():
            suggestions = [
                {
                    'file': 'test.py',
                    'type': 'xss',
                    'severity': 'high',
                    'description': 'XSS vulnerability found'
                },
                {
                    'file': 'requirements.txt',
                    'type': 'dependency',
                    'severity': 'medium',
                    'description': 'Outdated dependencies'
                }
            ]
            command = f"implement fixes: {suggestions}"
        elif 'git checkout -b' in command:
            command = 'create branch'
        elif 'gh pr create' in command:
            command = 'create pull request'
            
        return MagicMock(
            returncode=0,
            stdout=f"Test output for: {command}",
            stderr=""
        )
    mock_run.side_effect = mock_subprocess

    # Set up environment variables
    with patch.dict(os.environ, {
        'OPENAI_API_KEY': 'test-key',
        'ANTHROPIC_API_KEY': 'test-key',
        'CI': 'true',  # Remove webhook requirement in CI
        'SKIP_CACHE': 'true'
    }):
        # Run pipeline in CI mode
        result = pipeline.run_pipeline()
        assert isinstance(result, dict), "Pipeline should return results dict"
        assert result.get('status') is True, "Pipeline should succeed"
    
    # Verify expected commands were called
    expected_calls = [
        'security checks',
        'architecture review',
        'implement fixes',
        'create branch',
        'create pull request'
    ]
    
    # Extract actual commands from mock calls
    actual_commands = []
    for call in mock_run.call_args_list:
        command = ' '.join(str(x) for x in call[0][0])
        # Map commands to step names using the same logic as mock_subprocess
        if any(x in command for x in ['nuclei', 'zap', 'dependency-check']):
            actual_commands.append('security checks')
        elif 'aider' in command and 'Review' in command:
            actual_commands.append('architecture review')
        elif 'aider' in command and 'fix' in command.lower():
            actual_commands.append('implement fixes')
        elif 'git checkout -b' in command:
            actual_commands.append('create branch')
        elif 'gh pr create' in command:
            actual_commands.append('create pull request')

    # Verify each expected call is present
    for expected in expected_calls:
        assert expected in actual_commands, f"Missing CI step: {expected}"


def test_artifact_generation(pipeline, tmp_path):
    """Test security report artifact generation"""
    report_file = tmp_path / "security-report.md"

    # Run pipeline with artifact generation
    with patch.dict(os.environ, {
        'OPENAI_API_KEY': 'test-key',
        'ANTHROPIC_API_KEY': 'test-key',
        'SLACK_WEBHOOK': 'https://example.com/webhook'
    }):
        results = pipeline.run_pipeline()
    pipeline.generate_review_report(results, report_file)
    
    # Verify artifact was created
    assert report_file.exists(), "Security report not generated"
    
    # Verify report content
    content = report_file.read_text()
    required_sections = [
        "# Security Review Report",
        "## Findings",
        "## Recommendations"
    ]
    for section in required_sections:
        assert section in content, f"Missing report section: {section}"

@patch('subprocess.run')
def test_cache_in_ci(mock_run, pipeline, tmp_path):
    """Test caching behavior in CI environment"""
    # Configure clean cache for test
    pipeline.cache = SecurityCache(str(tmp_path))
    
    # Mock successful command executions
    mock_run.return_value = MagicMock(
        returncode=0,
        stdout="Test output",
        stderr=""
    )

    # Set up CI environment
    with patch.dict(os.environ, {
        'CI': 'true', 
        'OPENAI_API_KEY': 'test-key',
        'ANTHROPIC_API_KEY': 'test-key'
    }):
        # First run - should skip cache in CI
        first_results = pipeline.run_pipeline()
        assert isinstance(first_results, dict), "Pipeline should return results dict"
        assert first_results.get('status') is True, "Pipeline should succeed"
        
        # Verify cache behavior in CI
        cache_files = list(Path(tmp_path).glob('**/*.json'))
        assert len(cache_files) == 0, "Cache should not be created in CI"
        
        # Second run - should also skip cache
        second_results = pipeline.run_pipeline()
        assert isinstance(second_results, dict), "Pipeline should return results dict"

@pytest.mark.parametrize('test_file,expected_findings', [
    ('sql_injection.py', ['sql_injection']),
    ('command_injection.py', ['command_injection']), 
    ('xss_vulnerability.py', ['xss']),
    ('crypto_weak.py', ['weak_crypto']),
    ('xss_complex.py', ['xss']),
    ('xss_dom.py', ['xss']),
    ('xss_stored.py', ['xss'])
])
def test_specific_vulnerability_detection(pipeline, tmp_path, test_file, expected_findings):
    """Test detection of specific vulnerability types"""
    # Create test files with specific vulnerabilities
    vulnerability_samples = {
        'sql_injection.py': '''
import sqlite3
def unsafe_query(user_id):
    query = f"SELECT * FROM users WHERE id = {user_id}"
    return query
''',
        'command_injection.py': '''
import os
def unsafe_command(user_input):
    os.system(f"echo {user_input}")
    return True
''',
        'xss_vulnerability.py': '''
from html import escape
def render_unsafe(user_input):
    html = f"<div>{escape(user_input)}</div>"
    return html
''',
        'crypto_weak.py': '''
import hashlib
def hash_password(password):
    return hashlib.md5(password.encode()).hexdigest()
''',
        'xss_complex.py': '''
def render_complex(user_input, user_data):
    from html import escape
    import json
    template = f"""
        <div class="user-content">
            <script>
                var userData = {json.dumps(user_data)};
                document.getElementById('content').textContent = {json.dumps(user_input)};
            </script>
        </div>
    """
    return template
''',
        'xss_dom.py': '''
def render_dom(user_input):
    from html import escape
    return f"""
        <div id="content"></div>
        <script>
            const sanitizedInput = (input) => {{
                const div = document.createElement('div');
                div.textContent = input;
                return div.textContent;
            }};
            document.getElementById("content").textContent = 
                sanitizedInput(location.hash.substring(1));
        </script>
    """
''',
        'xss_stored.py': '''
class UserProfile:
    def __init__(self):
        self.comments = []
        
    def add_comment(self, comment):
        self.comments.append(comment)
        
    def render_comments(self):
        from html import escape
        return "".join([
            f"<div class='comment'>{escape(comment)}</div>"
            for comment in self.comments
        ])
'''
    }
    
    # Create test file with proper content
    test_path = tmp_path / test_file
    test_path.write_text(vulnerability_samples[test_file])
    
    # Run review
    results = pipeline.review_paths([str(tmp_path)])
    assert 'reviews' in results, "Results should contain 'reviews' key"
    
    # Extract findings
    findings = []
    for review in results['reviews']:
        if isinstance(review.get('findings'), list):
            for finding in review['findings']:
                if isinstance(finding, dict) and 'type' in finding:
                    findings.append(finding['type'])
    
    # Verify each expected finding
    for expected in expected_findings:
        assert expected in findings, f"Failed to detect {expected} vulnerability in {test_file}"
